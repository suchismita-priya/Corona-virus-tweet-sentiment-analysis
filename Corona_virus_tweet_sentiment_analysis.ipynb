{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suchismita-priya/corona-virus-tweet-sentiment-analysis/blob/main/Corona_virus_tweet_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -** Suchismita Priyadarsinee"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This challenge asks you to build a classification model to predict the sentiment of COVID-19 tweets.The tweets have been pulled from Twitter and manual tagging has been done then.\n",
        "The names and usernames have been given codes to avoid any privacy concerns.\n",
        "You are given the following information:\n",
        "\n",
        "Location\n",
        "\n",
        "Tweet At\n",
        "\n",
        "Original Tweet\n",
        "\n",
        "Label"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This challenge asks you to build a classification model to predict the sentiment of COVID-19 tweets.The tweets have been pulled from Twitter and manual tagging has been done then.\n",
        "The names and usernames have been given codes to avoid any privacy concerns.\n",
        "You are given the following information:\n",
        "\n",
        "Location\n",
        "\n",
        "Tweet At\n",
        "\n",
        "Original Tweet\n",
        "\n",
        "Label"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Provide the file path to your CSV file\n",
        "file_path = '/content/drive/MyDrive/Coronavirus Tweets.csv'\n",
        "\n",
        "# Read the CSV file with the specified encoding\n",
        "df = pd.read_csv(file_path, encoding='latin')\n"
      ],
      "metadata": {
        "id": "KpXlyIQ4ZAvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "row_col_count = df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('row count : ', row_col_count[0])\n",
        "print('column count : ', row_col_count[1])"
      ],
      "metadata": {
        "id": "Lm4pAxO0qgRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dataset Duplicate Value Count\n",
        "len(df[df.duplicated()])"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "import missingno as msno\n",
        "msno.bar(df)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Location column has missing values and no other column has any missing values.\n"
      ],
      "metadata": {
        "id": "K6S_6LiH2gEh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this Dataset their are 6 column having 'Location', 'UserName', 'ScreenName', 'TweetAt', 'OriginalTweet','Sentiment' and 'Location' column have duplicated value i.e 8590 which can take action later on as proceed on our Classfication Machine Learning Project."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe(include='all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**UserName**: This column contains the username of the person who posted the tweet.\n",
        "\n",
        "**ScreenName**: This column contains the screen name or handle of the user who posted the tweet.\n",
        "\n",
        "**Location**: This column contains the location of the user who posted the tweet. This could be their city, state, country, or any other geographic location that they have specified in their Twitter profile.\n",
        "\n",
        "**TweetAt**: This column contains the date and time when the tweet was posted.\n",
        "\n",
        "**OriginalTweet**: This column contains the actual text of the tweet that was posted.\n",
        "\n",
        "**Sentiment**: This column contains the sentiment label assigned to the tweet. This label could be positive, negative,extremely positive,extremely negative neutral, depending on the sentiment analysis algorithm used to classify the tweet.\n",
        "\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "df.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "df['TweetAt'].dtype"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['TweetAt'] = pd.to_datetime(df['TweetAt'])"
      ],
      "metadata": {
        "id": "speqI2xGfGeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"unique years {df['TweetAt'].dt.year.unique()}\")\n",
        "print(f\"unique month {sorted(df['TweetAt'].dt.month.unique())}\")\n",
        "print(f\"unique day {sorted(df['TweetAt'].dt.day.unique())}\")\n",
        "print(f\"unique day {df['TweetAt'].dt.day_name().unique()}\")"
      ],
      "metadata": {
        "id": "gdNTNx3wfPEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Location'].unique()"
      ],
      "metadata": {
        "id": "F8LLbRfM11mX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Location column has missing values and no other column has any missing values.\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "sns.countplot(data=df, x='Sentiment')\n",
        "plt.title('Count of Sentiments')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific chart used in the code is a countplot, which is essentially a bar chart that represents the counts of unique values in a categorical variable. In this case, it's used to visualize the distribution of different sentiments in the dataset.\n",
        "\n",
        "Countplots are a suitable choice for visualizing the distribution of categorical variables, such as sentiment labels in this context, because they provide a clear and concise representation of how many instances belong to each category. This visualization is helpful when you want to understand the balance or distribution of sentiment labels in your dataset."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Most tweets are positive, followed by negative and neutral.\n",
        "- Least number of tweets are extremely negative."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most tweets express positive sentiment, followed by negative and neutral:\n",
        "\n",
        "**Positive Impact:** The prevalence of positive sentiment in tweets related to the coronavirus indicates that the majority of people may hold a positive outlook or hope, possibly reflecting positive developments such as vaccine news, recoveries, or community support during the pandemic.\n",
        "\n",
        "**Business Action:** Organizations can leverage this positive sentiment to promote public health measures, support initiatives, or share positive stories to maintain a hopeful outlook. This can encourage public cooperation with health guidelines.\n",
        "\n",
        "\n",
        "Least number of tweets express extremely negative sentiment:\n",
        "\n",
        "**Positive Impact:** A smaller number of extremely negative tweets suggests that, in general, there may not be widespread panic or despair regarding the coronavirus. It could indicate that the situation is relatively under control or that people are not experiencing extreme negativity.\n",
        "\n",
        "**Business Action:** Organizations can continue to provide accurate information, support resources, and safety measures. By addressing concerns and maintaining transparency, they can help prevent extreme negativity.\n",
        "In the context of the coronavirus, the insights gained from sentiment analysis can be valuable for public health authorities, organizations, and businesses. These insights can help them tailor their messaging, resources, and support to ensure that the public remains informed and maintains a balanced perspective on the situation."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Looking into the count value of different Location\n",
        "df.Location.value_counts().head(15)"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Looking for the unique values in the variable\n",
        "df.Location.unique()"
      ],
      "metadata": {
        "id": "7eaaK1p632E_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Location'].value_counts()[0:15].plot(kind='bar')"
      ],
      "metadata": {
        "id": "hKkuPuG14AHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts make it easy to compare different categories or groups of data. You can quickly see which category has the highest or lowest value. bar charts are a versatile and widely used tool for data visualization, particularly when dealing with categorical or discrete data.\n",
        "\n",
        "They provide a clear visual representation of data, making it easier to draw insights and make informed decisions."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the tweets are from UK(London) , USA(New york, Washington dc,Los angeles) and India."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above count plot shows the top 15 locations in the dataset.\n",
        "\n",
        "Plot shows the London to be the maximum among all the different locations.\n",
        "\n",
        "Second place is of United States and India ranks at 8th place."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "# Count value of TweetAt (Tweeting date)\n",
        "df['TweetAt'].value_counts()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.TweetAt.unique()"
      ],
      "metadata": {
        "id": "6h7jc4UNSmfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of Dates of Tweets\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.countplot(x='TweetAt', data=df, palette ='Dark2')\n",
        "plt.title(\"Tweeting Dates\")\n",
        "plt.xticks(rotation=45,ha='right')\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xlabel(\"TweetAt\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xEy7DWAYSySk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bar plot is a common and widely used chart type for visualizing categorical data, such as dates or categories, with a discrete count or frequency associated with each category."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The above count plot shows the overall distribution of different tweeting dates in the dataset. Starting from the mid-month of March 2020 to mid-month of April 2020.\n",
        "\n",
        "- From the we can find out that the date 20-03-2020 has the maximum count value among all the other occuring dates.\n",
        "\n",
        "- Least count value is for date 28-03-2020.\n",
        "\n",
        "- The tweeting date ranges from 16-03-2020 to 14-04-2020, which is approx 30 days in total."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.. **Peak on 20-03-2020:**\n",
        "\n",
        "**Positive Impact:** The peak in tweet activity on this specific date could be due to a significant event, announcement, or discussion related to the coronavirus. Businesses that are actively monitoring and engaging with the public on such dates may have the opportunity to reach a larger audience.\n",
        "\n",
        "**Business Action:** Organizations can use this insight to identify critical dates and tailor their communication or marketing efforts around them.\n",
        "\n",
        "2.. **Least Activity on 28-03-2020:**\n",
        "\n",
        "**Neutral Impact:** The lower tweet activity on this date might not indicate a negative impact directly. It could be a day with lower news or events related to the coronavirus.\n",
        "\n",
        "**Business Action:** Organizations can plan their social media and communication strategies, considering the fluctuation in public engagement.\n",
        "\n",
        "\n",
        "3.. **Tweeting Date Range: 16-03-2020 to 14-04-2020:**\n",
        "\n",
        "**Neutral Impact:** The continuous tweeting activity over approximately 30 days signifies a sustained interest or discussion about the topic.                     \n",
        "**Business Action:** Organizations can maintain their presence and keep the public informed during extended periods of discussion."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "df['text_length'] = df['OriginalTweet'].apply(len)"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_character_length=df['text_length'].sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "HnnK8IRFkPpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_character_length"
      ],
      "metadata": {
        "id": "s32Dvz3bk4Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame `top_tweet_character_length` from `tweet_character_length\n",
        "top_tweet_character_length=pd.DataFrame(tweet_character_length)\n",
        "# Reset the index of `top_tweet_character_length` and assign the result back to `top_tweet_character_length`\n",
        "top_tweet_character_length.reset_index(inplace=True)\n",
        "# Rename the columns of `top_tweet_character_length` to 'Original_Tweet_Row' and 'tweet_character_Count'\n",
        "top_tweet_character_length.rename(columns={'index':'Original_Tweet_Row', 'text_length':'tweet_character_Count'}, inplace=True)"
      ],
      "metadata": {
        "id": "QrZn8wl1kbat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_tweet_character_length"
      ],
      "metadata": {
        "id": "P6EYFCz3lRe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_tweet_length=top_tweet_character_length.head(10).sort_values(by='tweet_character_Count',ascending=False)"
      ],
      "metadata": {
        "id": "sjrx4M9FlcqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(x=\"Original_Tweet_Row\", y=\"tweet_character_Count\", data=top_tweet_length, palette='viridis')"
      ],
      "metadata": {
        "id": "3JQgIh_QlohC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To visualize this data, a bar chart is a good choice. A bar chart can effectively show the Original_Tweet_Row from each location in a clear and concise way. Each location can be represented by a bar, with the height of the bar indicating the tweet_character_Count."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Row Number 25160 has highest Tweet length of character i.e 350"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Potential Insights:**\n",
        "\n",
        "**High Engagement or Elaboration:** A tweet with a longer character count might indicate that the user is providing more detailed information, explanations, or comments. This could be a positive sign, as it might suggest higher engagement with the topic or content.\n",
        "\n",
        "**Negative Growth or Challenges:** On the flip side, a tweet with an exceptionally long character count might also indicate a negative scenario, such as a complex issue, a complaint, or a crisis. Lengthy tweets are sometimes used to express frustration or dissatisfaction.\n",
        "\n",
        "**Business Actions:**\n",
        "\n",
        "**Positive Engagement:** Businesses can monitor tweets with longer character counts for positive engagement. If users are elaborating on their positive experiences or expressing a deep interest in a product or service, this could be leveraged for marketing and building a positive brand image.\n",
        "\n",
        "**Negative Issues and Crisis Management:** On the other hand, if the longer tweets are related to negative experiences or problems, it's crucial for businesses to identify and address these concerns promptly. Negative tweets can impact the brand's reputation, so addressing issues and resolving complaints can mitigate negative consequences."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Tweet Count For Each Sentiment\n",
        "sentiment_count = df['Sentiment'].value_counts().reset_index()\n",
        "sentiment_count.columns = ['Sentiment','count']\n",
        "sentiment_count"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of \"Positive\" sentiments are higher than all other sentiments"
      ],
      "metadata": {
        "id": "5CvD3q-8nGYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# replace values\n",
        "replace_values = {'Sentiment':{'Extremely Positive':'Positive','Extremely Negative':'Negative'}}\n",
        "df=df.replace(replace_values)"
      ],
      "metadata": {
        "id": "0wYMDfninJb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_count1 = df['Sentiment'].value_counts().reset_index()\n",
        "sentiment_count1.columns = ['Sentiment','count']\n",
        "sentiment_count1"
      ],
      "metadata": {
        "id": "Mw1B-cqUnxfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the piechart for Sentiments distribution\n",
        "sentiment_count1 = df['Sentiment'].value_counts().to_list()\n",
        "labels=['Positive','Negative','Netural']\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.pie(x=sentiment_count1,shadow= True,labels=labels,autopct=\"%.2f%%\",radius=1.1)\n",
        "plt.title(\"Proportion Of Sentiments\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-zSWhrkTn5s4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " It is chosen for visualizing the distribution of sentiments (positive, negative, and neutral) because of its ability to effectively convey the proportion or composition of different categories in a dataset."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We combined Extremly Positive and Negative sentiments to positive and negative sentiments respectively. As we can observe on the Pie-Plot, The total number of \"Positive\" sentiments are still high after combining."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Business Impact:**\n",
        "\n",
        "Simplification: Combining \"Extremely Positive\" and \"Extremely Negative\" sentiments into their respective broader categories simplifies the sentiment analysis process. It reduces the number of sentiment categories, making it easier to categorize and analyze sentiment.\n",
        "\n",
        "More Balanced Distributions: The Pie-Plot observation that the total number of \"Positive\" sentiments is still high after combining suggests that the dataset might have had an imbalanced distribution of extremely positive and extremely negative sentiments. By combining them with their broader categories, the sentiment distribution may become more balanced, which can lead to more reliable sentiment analysis results.\n",
        "\n",
        "User Experience: From a business perspective, this simplification might lead to a more straightforward user experience when presenting sentiment analysis results. It's often easier to explain and act upon positive and negative sentiments rather than distinguishing between extreme and non-extreme sentiments.\n",
        "\n",
        "**Negative Business Impact:**\n",
        "\n",
        "Loss of Fine-Grained Analysis: Combining \"Extremely Positive\" and \"Extremely Negative\" sentiments into broader categories results in a loss of granularity. This means that the analysis might miss nuances present in the extreme sentiments. For some businesses, understanding the subtleties in sentiment can be crucial for decision-making.\n",
        "\n",
        "Potentially Biased Insights: The decision to combine sentiments should be made carefully. If the dataset originally had a significant number of extremely positive or extremely negative sentiments, merging them into broader categories may lead to biased or oversimplified insights."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "df['month'] = df['TweetAt'].dt.month\n",
        "df['day'] = df['TweetAt'].dt.day\n",
        "df['day_name'] = df['TweetAt'].dt.day_name()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(data=df, x='month', color='blue')\n",
        "plt.title('Count of Months')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "x1z-6mrgwRbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This displays the distribution of tweet counts across different months."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Most tweets are from March.\n",
        "- Corona varius was decleared pandamic in March."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Implications:\n",
        "\n",
        "Public Engagement: The fact that most tweets are from March suggests a high level of public engagement during that period. This could be an indicator of people actively discussing and sharing information about the pandemic.\n",
        "\n",
        "Awareness and Information Sharing: March was a critical time for disseminating information about COVID-19. The high tweet activity during this month could indicate that people were using social media platforms to share important information, guidelines, and updates related to the pandemic. This can be seen as a positive sign of public awareness and information sharing.\n",
        "\n",
        "\n",
        "Negative Implications:\n",
        "\n",
        "Pandemic Severity: On the negative side, the high tweet activity in March might also indicate the severity of the pandemic during that time. A surge in tweets could imply that the situation was escalating, which could be a cause for concern.\n",
        "\n",
        "Negative Sentiments: The high tweet activity could also include negative sentiments, such as fear, anxiety, or anger. A large number of tweets about a pandemic can be indicative of public distress."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(data=df, x='day')\n",
        "plt.title('Count of Days')\n",
        "plt.xlabel('Day')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This displays the distribution of tweet counts across different days."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- from chart it is clear on first week specifically on 4th march the cases were high.\n",
        "- It got decresed but again showed up during mid month during 16th march to 25th march.\n",
        "- after that cases were very less could be 10% or 5%."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Business Impact:**\n",
        "\n",
        "Healthcare and Pharmaceutical Industries: For businesses in the healthcare and pharmaceutical sectors, fluctuations in COVID-19 cases can drive demand for medical supplies, medications, and healthcare services. High case counts might lead to increased demand for testing, treatment, and preventive measures. These industries could potentially benefit from the observed fluctuations.\n",
        "\n",
        "Delivery Services and Online Retail: During periods of high cases or lockdowns, people may increase their reliance on online shopping and delivery services. Businesses in e-commerce and delivery might see higher demand during these times.\n",
        "\n",
        "\n",
        "**Negative Business Impact:**\n",
        "\n",
        "Brick-and-Mortar Retail: Physical retailers, especially those in non-essential categories, might see negative impacts during lockdowns or when cases are high. Reduced foot traffic and consumer reluctance to visit stores could lead to decreased sales.\n",
        "\n",
        "Hospitality and Travel: Industries like hospitality and travel are often negatively affected during periods of high COVID-19 cases or lockdowns. People may cancel travel plans and avoid hotels and restaurants, resulting in reduced revenue.\n",
        "\n",
        "Event and Entertainment: Businesses involved in organizing events, concerts, and entertainment might face challenges during periods of high cases. Cancellations or reduced attendance at events could have negative financial consequences.\n",
        "\n",
        "Manufacturing and Supply Chains: Global supply chains can be disrupted by lockdowns, quarantine measures, and reduced workforce availability. Manufacturing businesses may face challenges in sourcing materials and fulfilling orders."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "sns.catplot(x= 'day', data=df, col='month',kind='count',col_wrap=4,sharey=False,sharex=False);"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The choice of using a catplot in this specific scenario appears to be motivated by the need to visualize the count of tweets over different days of the week, across different months."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Only in march tweets are there are most number of tweets from 4th to 31st, with 20th being the day when most number of tweets occured.\n",
        "- april had tweets from 4th,13th and 14th while most number of tweets occured on 13th.\n",
        "- All other months have tweets only from 4th of the repective month."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Business Impact:**\n",
        "\n",
        "March Surge: The observation that there's a surge in tweets in March, particularly from the 4th to the 31st, with the highest tweet count on the 20th, could be indicative of a significant public interest or engagement during that time. If a business or organization was actively involved in providing relevant information, services, or products related to the context of the tweets (e.g., COVID-19 information, products, or services), this could have been a valuable period for engagement and outreach.\n",
        "\n",
        "April Activity: While April had fewer tweets than March, there's still some activity observed from the 4th, 13th, and 14th, with a peak on the 13th. For businesses that can adapt and provide timely responses, this might have represented an opportunity to continue engagement.\n",
        "\n",
        "\n",
        "**Negative Growth:**\n",
        "\n",
        "Limited Data in Other Months: In contrast, the insight that all other months have tweets starting from the 4th of the respective month and relatively low tweet counts could indicate limited activity during those times. For a business that depends on social media engagement or online activity, these periods might represent times of reduced public interaction."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(data=df, x='day_name')\n",
        "plt.title('Count of Days name')\n",
        "plt.xlabel('Day name')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Showing chart according to days."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most tweets happended during wednesday and least during sunday\n",
        "\n"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Business Impact:**\n",
        "\n",
        "Wednesday Engagement: The observation that most tweets occurred on Wednesdays could be indicative of higher social media activity during the middle of the workweek. For businesses or organizations actively involved in social media marketing or outreach, this could present an opportunity to engage with a larger audience. Businesses that adapt their strategies to focus on Wednesdays might see increased reach and engagement during those periods.\n",
        "\n",
        "Other Active Days: It's also worth noting that other weekdays may have relatively high tweet activity. If a business has the capacity to maintain a consistent social media presence during these days, it could lead to more opportunities for customer engagement and communication.\n",
        "\n",
        "**Negative Growth:**\n",
        "\n",
        "Sunday Inactivity: The insight that there are fewer tweets on Sundays could indicate lower online engagement during weekends, especially Sundays. For businesses that depend heavily on weekend engagement, this might represent a less active period.\n",
        "\n",
        "Consistency Challenges: If a business is unable to maintain a consistent online presence on the days with higher tweet activity (e.g., Wednesdays), it might miss out on potential engagement opportunities."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "sns.catplot(x= 'day_name', data=df, col='month',kind='count',col_wrap=4,sharey=False,sharex=False)"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "catplot with days of the week on the x-axis and counts of tweets on the y-axis, organized by month, is an effective choice for visualizing tweet patterns"
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On march and april month patterns are showing."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tweet Patterns in March:\n",
        "\n",
        "Increasing Activity: Tweet activity appears to increase as the month progresses, with the highest number of tweets occurring around the 20th of March. This might correspond to a significant event or announcement related to the coronavirus, leading to heightened social media discussions.\n",
        "Business Impact:\n",
        "\n",
        "Awareness and Engagement: If your business is involved in healthcare, public health, or any sector directly impacted by the coronavirus, this increased awareness and engagement could be an opportunity to share information or promote relevant products or services."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "sns.boxplot(data=df, y='month',x='Sentiment')"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "boxplot to visualize the distribution of sentiments across different months in the dataset provides several benefits:\n",
        "\n",
        "Comparison of Sentiments: A boxplot effectively displays the distribution of sentiment scores (positive, negative, neutral) across different months, making it easy to compare how sentiments vary over time.\n",
        "\n",
        "Outlier Detection: Boxplots can identify potential outliers within each sentiment category for each month. Outliers could represent extreme sentiment scores that might require specific attention in further analysis."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- showing more outliers in negative tweets.\n",
        "- positive sentiments are showing less outliers than negative."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Monitoring and Managing Negative Sentiments: The higher number of outliers in negative tweets may indicate a stronger emotional response from users. Businesses should closely monitor and address negative sentiments as they can quickly escalate and damage a company's reputation. Promptly addressing these negative sentiments can mitigate the impact on the brand.\n",
        "\n",
        "Opportunity for Positive Engagement: With fewer outliers in positive sentiments, there may be a less critical need for immediate responses. However, it's an opportunity for businesses to engage with their satisfied customers and potentially turn them into brand advocates. Acknowledging and appreciating positive feedback can strengthen customer loyalty and word-of-mouth marketing.\n",
        "\n",
        "Risk Assessment: The presence of outliers, especially in negative sentiments, can be an early warning sign of potential issues. Businesses can use sentiment analysis to identify and assess risks, allowing them to proactively address customer concerns or improve their products and services.\n",
        "\n",
        "Improving Customer Service: Identifying and addressing outliers in sentiment can help businesses improve their customer service. If certain products or services consistently receive negative feedback, it's an opportunity to make necessary improvements and enhance the customer experience.\n",
        "\n",
        "Marketing Strategy: Understanding the sentiment distribution and outliers can inform marketing strategies. Businesses can tailor their campaigns to target specific sentiment groups, addressing concerns or emphasizing positive aspects of their offerings.\n",
        "\n",
        "Competitor Analysis: Analyzing sentiment outliers in comparison to competitors can provide insights into market positioning. Identifying areas where a business excels or falls behind competitors can guide strategic decisions."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "sns.violinplot(data=df, y='month',x='Sentiment')"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A violin plot is used when you want to visualize the distribution of a numerical variable across different categories or groups. In this case, the plot is showing the distribution of sentiment scores across different months."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As like above same outliers are showing."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "like boxplot these are also providing same business impact."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_vars = list(df.select_dtypes(exclude=['object']))\n",
        "num_vars"
      ],
      "metadata": {
        "id": "6psUD6Yy9Cxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "for i in num_vars:\n",
        "  sns.histplot(x = df[i])\n",
        "  plt.axvline(df[i].mean(), color='g', linestyle='dashed', linewidth=2)\n",
        "  plt.axvline(df[i].median(), color='red', linestyle='dashed', linewidth=2)\n",
        "  plt.title(i)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "it is creating histogram plots for numerical variables in your dataset and overlaying vertical lines for the mean (in green) and the median (in red) of each variable. This is a common approach for visualizing the distribution and central tendency of numerical data."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Central Tendency: The vertical green line represents the mean, while the red line represents the median. Comparing these lines can help you assess the central tendency of the data. If the mean and median are close, it suggests a symmetric distribution. If they differ significantly, it might indicate the presence of outliers.\n",
        "\n",
        "Skewness: The shape of the histogram can reveal the presence of skewness. A long tail to the right (positive skew) or left (negative skew) indicates an asymmetric distribution.\n",
        "\n",
        "Outliers: Histograms can help identify potential outliers. Outliers are data points that deviate significantly from the majority of the data. They are often located in the tails of the distribution.\n",
        "\n",
        "- textlength ,month,day is having symmetric distribution while others are not.\n",
        "- positive skewed are day,month,tweet at.\n",
        "- negetive skewed are textlength."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Insights:\n",
        "\n",
        "Symmetric Distributions: Variables like text_length, month, and day showing symmetric distributions are generally easier to work with in statistical analyses. This symmetry can simplify modeling and analysis.\n",
        "Negative Insights:\n",
        "\n",
        "Positive Skewness: Variables like day, month, and tweet_at exhibit positive skewness. This suggests that these variables are concentrated on the lower side of their respective ranges. In a business context, this could imply that certain days, months, or times are associated with lower activity or fewer tweets. Understanding why these periods have lower engagement or activity could be important for a business.\n",
        "\n",
        "Negative Skewness: The text_length variable displays negative skewness. In this case, most tweets have longer text lengths, but there are some tweets with exceptionally short text. Negative skewness might indicate that the majority of tweets contain more information. Understanding this could be valuable for content strategy."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "plt.figure(figsize=(20,8))\n",
        "sns.heatmap(df.corr(),annot=True,cmap='coolwarm')"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is particularly useful for visualizing complex datasets, especially when you want to explore relationships between variables or patterns within the data. Heatmaps are excellent for visualizing relationships or correlations between variables in a dataset. By assigning colors to different values, you can quickly identify which variables are positively or negatively correlated."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- username,screenname are positively corelated with each other."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(df)\n",
        "plt.figure(figsize=(30,25))"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "pairplot is a type of data visualization that is particularly useful when dealing with datasets that contain multiple numerical features (variables). It provides a grid of scatterplots for each pair of numerical variables in your dataset, allowing you to visualize the relationships and correlations between them."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "username & screenname are highly corelated to each other, so in future at time of model training we'll use only one ."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis 1:\n",
        "Null Hypothesis (H0): The average sentiment score for tweets about coronavirus is equal among different months.\n",
        "Alternative Hypothesis (H1): The average sentiment score for tweets about coronavirus is not equal among different months.\n",
        "\n",
        "Hypothesis 2:\n",
        "Null Hypothesis (H0): The sentiment of tweets about coronavirus is the same for different countries.\n",
        "Alternative Hypothesis (H1): The sentiment of tweets about coronavirus varies among different countries.\n",
        "\n",
        "Hypothesis 3:\n",
        "Null Hypothesis (H0): The sentiment of tweets about coronavirus is the same for tweets posted from different days.\n",
        "Alternative Hypothesis (H1): The sentiment of tweets about coronavirus differs among tweets posted from different days."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H0): The average sentiment score for tweets about coronavirus is equal among different months.\n",
        "\n",
        "Alternative Hypothesis (H1): The average sentiment score for tweets about coronavirus is not equal among different months."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import f_oneway"
      ],
      "metadata": {
        "id": "S6jTrzLIBp9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Create a dictionary to map sentiment categories to numerical values\n",
        "sentiment_mapping = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
        "\n",
        "# Apply the mapping to create a new column with numerical sentiment values\n",
        "df['Sentiment_Num'] = df['Sentiment'].map(sentiment_mapping)\n",
        "\n",
        "# Group the data by month and get the numerical sentiment scores\n",
        "groups = [group[\"Sentiment_Num\"] for name, group in df.groupby(\"month\")]\n",
        "# Perform one-way ANOVA\n",
        "f_statistic, p_value = f_oneway(*groups)\n",
        "\n",
        "# Set your significance level (alpha)\n",
        "alpha = 0.05\n",
        "\n",
        "# Check if the p-value is less than alpha\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis. There are significant differences among the months.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis. There are no significant differences among the months.\")"
      ],
      "metadata": {
        "id": "H1I61tx3qReE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistical Test: One-way Analysis of Variance (ANOVA)"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific statistical test, a t-test, was chosen because it is appropriate for comparing the means of two groups to determine if they are significantly different from each other."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this statement, the null hypothesis assumes that there is no significant difference in the sentiment of tweets about coronavirus among different countries.\n",
        "\n",
        "The alternative hypothesis, on the other hand, suggests that there are significant variations in the sentiment of tweets across different countries. To test this hypothesis, we can use statistical tests or analysis of variance (ANOVA) to examine whether sentiment scores significantly differ among countries."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats"
      ],
      "metadata": {
        "id": "z6EGqretKJpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Map sentiment labels to numeric scores\n",
        "sentiment_scores = {'Positive': 2, 'Neutral': 1, 'Negative': 0}\n",
        "\n",
        "# Apply the mapping to create a new 'Sentiment_Score' column\n",
        "df['Sentiment_Score'] = df['Sentiment'].map(sentiment_scores)\n",
        "\n",
        "# Define a list of locations to compare\n",
        "locations = ['London','United States','London','England','New York',\n",
        "             'NY','Washington', 'DC','United Kingdom','Los Angeles', 'CA','India','UK','Australia','USA','Canada','England', 'United Kingdom','Toronto','Ontario','Global']\n",
        "# Initialize an empty list to store the sentiment scores for each location\n",
        "sentiment_groups = []\n",
        "\n",
        "# Loop through the locations and extract sentiment scores\n",
        "for location in locations:\n",
        "    sentiment_group = df[df['Location'] == location]['Sentiment_Score']\n",
        "    sentiment_groups.append(sentiment_group)\n",
        "\n",
        "# Perform ANOVA\n",
        "f_statistic, p_value = stats.f_oneway(*sentiment_groups)\n",
        "\n",
        "# Print the results\n",
        "print(\"F-statistic:\", f_statistic)\n",
        "print(\"P-value:\", p_value)\n",
        "\n",
        "# Define the significance level (alpha)\n",
        "alpha = 0.05\n",
        "\n",
        "# Check if p-value is less than alpha to determine significance\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis\")\n",
        "    print(\"There are significant differences in sentiment among locations.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis\")\n",
        "    print(\"There are no significant differences in sentiment among locations.\")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the one-way Analysis of Variance (ANOVA) test to obtain a p-value. ANOVA is a statistical test used to compare the means of multiple groups to determine whether there are statistically significant differences among those groups.\n",
        "\n",
        "In the context of the previous discussion, I performed ANOVA to test whether there were statistically significant differences in sentiment scores among different locations. The p-value obtained from ANOVA helps in making a decision about whether there are significant differences or not."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the one-way Analysis of Variance (ANOVA) test in this context because it's suitable for comparing the means of multiple groups. The hypothesis we were testing was whether the sentiment of tweets about coronavirus varies among different countries (locations). Since we had more than two groups (different countries), ANOVA was an appropriate choice.\n",
        "\n",
        "ANOVA evaluates whether there are any statistically significant differences among the group means. If the p-value obtained from ANOVA is less than a chosen significance level (alpha), it suggests that there are significant differences in sentiment among countries. If the p-value is greater than alpha, it suggests there are no significant differences.\n",
        "\n",
        "In this case, the choice of statistical test is based on the specific research question and the type of data we are analyzing, which involves comparing sentiment scores (a continuous variable) among multiple groups (countries)."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H0): The sentiment of tweets about coronavirus is the same for tweets posted from different days.\n",
        "\n",
        "Alternative Hypothesis (H1): The sentiment of tweets about coronavirus differs among tweets posted from different days."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Create a contingency table\n",
        "contingency_table = pd.crosstab(df['Sentiment'], df['day'])\n",
        "\n",
        "# Perform the chi-squared test\n",
        "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "# Print the results\n",
        "print(\"Chi-squared:\", chi2)\n",
        "print(\"P-value:\", p)\n",
        "\n",
        "# Define the significance level (alpha)\n",
        "alpha = 0.05\n",
        "\n",
        "# Check if p-value is less than alpha to determine significance\n",
        "if p < alpha:\n",
        "    print(\"Reject the null hypothesis\")\n",
        "    print(\"There is a significant association between Sentiment and Days.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis\")\n",
        "    print(\"There is no significant association between Sentiment and Days.\")\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used the Chi-squared test for independence to obtain the p-value in my previous response. This test is used to determine whether there is a significant association between two categorical variables, in this case, the association between \"Sentiment\" and \"Days\" in the context of tweets about coronavirus.\n",
        "\n",
        "The p-value from the Chi-squared test helps assess the significance of this association."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the Chi-squared test for independence in this scenario because it is an appropriate statistical test to determine whether there is a significant association between two categorical variables, which is the case when investigating the relationship between \"Sentiment\" and \"Days\" in tweets about coronavirus. This test is commonly used for such analyses when working with categorical data.\n",
        "\n",
        "The null hypothesis (H0) assumes that there is no association between the two variables, while the alternative hypothesis (H1) assumes that there is a significant association.\n",
        "\n",
        "The Chi-squared test calculates the p-value, which tells us whether the observed associations are likely to have occurred by random chance. If the p-value is less than a chosen significance level (e.g., 0.05), we can reject the null hypothesis and conclude that there is a significant association between the variables. This test is suitable for assessing whether the sentiment of tweets differs among tweets posted from different devices."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace missing values with the most frequent value\n",
        "df = df.fillna(df.mode().iloc[0])"
      ],
      "metadata": {
        "id": "YHBK35RkFyuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "4gUCG-HwLkea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here missing values are replaced with the mode value or most frequent value of the entire feature column. When the data is skewed, it is good to consider using mode values for replacing the missing values.\n",
        "\n",
        "Imputing missing data with mode values can be done with numerical and categorical data."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_vars"
      ],
      "metadata": {
        "id": "Lq78Lx5WVFyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "0GNlUQXcVXC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2=df.copy()"
      ],
      "metadata": {
        "id": "qRuvF4d4VxeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.drop('TweetAt',axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "j3NAl0iUV1S8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_vars2 = list(df2.select_dtypes(exclude=['object']))\n",
        "num_vars2"
      ],
      "metadata": {
        "id": "fkUNhnBLWU6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "for i in num_vars2:\n",
        "  sns.boxplot(df2[i])\n",
        "  plt.xlabel(i)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# clipping method\n",
        "def clip_outliers(df2):\n",
        "  for i in df2[num_vars2]:\n",
        "    # using IQR method to define range of upper and lower limit.\n",
        "    q1 = df2[i].quantile(0.25)\n",
        "    q3 = df2[i].quantile(0.75)\n",
        "    iqr = q3-q1\n",
        "    lowerbound = q1-1.5*iqr\n",
        "    upperbound = q3+1.5*iqr\n",
        "    # replacing the outliers with upper and lower bound\n",
        "    df2[i] = df2[i].clip(lowerbound,upperbound)\n",
        "  return df2"
      ],
      "metadata": {
        "id": "tBY8gnkezD5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = clip_outliers(df2)"
      ],
      "metadata": {
        "id": "ICGqLbjzzSbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in num_vars2:\n",
        "  sns.boxplot(x=df2[i])\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "gAbzEEMfzVJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we have limited datapoint hence we are not simply removing the outlier instead of that we are using the clipping method."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "category_columns = list(df2.select_dtypes(include=object))\n",
        "category_columns"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in category_columns:\n",
        "  print('no of unique values in',i,'is',df2[i].nunique())"
      ],
      "metadata": {
        "id": "U2QFSX0tYcd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2_day = pd.get_dummies(df2[\"day_name\"])"
      ],
      "metadata": {
        "id": "1U4fVTQSzzrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.concat([df2, df2_day], axis=1)"
      ],
      "metadata": {
        "id": "QZrUiM9H0SKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.drop('day_name', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "HfwA6rxLhhCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.head(2)"
      ],
      "metadata": {
        "id": "C9AwYLyC3YGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Location and OriginalTweet Columns:** It's not common to perform one-hot encoding on text columns such as 'Location' or 'OriginalTweet'. Instead, text data is typically preprocessed, which may include text cleaning, tokenization, and vectorization using techniques like TF-IDF (Term Frequency-Inverse Document Frequency) or Word Embeddings like Word2Vec or GloVe. These techniques convert text data into a numerical format, which can then be used for modeling.\n",
        "\n",
        "**Sentiment Column:** Since the 'Sentiment' column has only three unique values, we can safely perform one-hot encoding, but as we'll use this in future so we'll keep this as it is."
      ],
      "metadata": {
        "id": "70wvRN4BZzmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2.columns"
      ],
      "metadata": {
        "id": "Yy_ucuzebHjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.head()"
      ],
      "metadata": {
        "id": "yNeO6Fo0ZBrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.shape"
      ],
      "metadata": {
        "id": "nBndPg2bcE2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**One-Hot Encoding:**\n",
        "Nominal Categorical Data: One-hot encoding is typically used for nominal categorical data, where there is no intrinsic order among categories. It creates binary columns (0 or 1) for each category, indicating the presence or absence of that category for each data point.\n",
        "\n",
        "**No Assumption of Order:** One-hot encoding doesn't assume any ordinal relationship between categories. It treats each category as a separate and independent feature.\n",
        "\n",
        "**Algorithm Compatibility**: Some machine learning algorithms, especially those based on distance or magnitude (e.g., k-means clustering), work better with one-hot encoded categorical features. These algorithms might misinterpret label-encoded features as having an ordinal relationship.\n",
        "\n",
        "**Interpretability:** One-hot encoding makes the data more interpretable because each category is explicitly represented by its own column."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install contractions"
      ],
      "metadata": {
        "id": "91lxwbyTf288"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "import contractions"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['OriginalTweet'] = df2['OriginalTweet'].apply(lambda x: contractions.fix(x))"
      ],
      "metadata": {
        "id": "0IjzZUaVkBQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.head()"
      ],
      "metadata": {
        "id": "9uIvTwmWkNgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "df2['OriginalTweet'] = df2['OriginalTweet'].str.replace('\\n',' ').str.lower()"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.head()"
      ],
      "metadata": {
        "id": "T6pg2xdwmLUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "# create function for punctuation removal:\n",
        "def remove_punctuations(text):\n",
        "    for char in string.punctuation:\n",
        "        text = text.replace(char, '')\n",
        "    return text"
      ],
      "metadata": {
        "id": "F6o0yNNQnNkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "df2['OriginalTweet'] = df2['OriginalTweet'].apply(remove_punctuations)"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.head()"
      ],
      "metadata": {
        "id": "NNbK7ZGjnWyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import nltk\n",
        "import re"
      ],
      "metadata": {
        "id": "GUuCTqIXppq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "df2['OriginalTweet'] = df2['OriginalTweet'].str.replace('http\\S+|www.\\S+', '', case=False)"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.head()"
      ],
      "metadata": {
        "id": "pra84Uckqx9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['OriginalTweet'] = df2['OriginalTweet'].str.replace(\"[^a-zA-Z#//]\",\" \")\n",
        "df2.head()"
      ],
      "metadata": {
        "id": "jmNSRh02rH1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "u-QnlUlJrzoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop = stopwords.words('english')"
      ],
      "metadata": {
        "id": "PR_fovowsAC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "df2['OriginalTweet'] = df2['OriginalTweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install neattext"
      ],
      "metadata": {
        "id": "xB7DQ_RFtOQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import neattext.functions as nfx"
      ],
      "metadata": {
        "id": "d2wZLWwFtJ3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['clean_Tweet'] = df2['OriginalTweet'].apply(nfx.remove_multiple_spaces)"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.head(2)"
      ],
      "metadata": {
        "id": "Rr48cEX2tWdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['new_Tweet'] = df2['clean_Tweet']\n",
        "df2.head(1)"
      ],
      "metadata": {
        "id": "Gm44D72Xyt2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['OriginalTweet'] = df2['OriginalTweet'].apply(lambda text: word_tokenize(text))\n",
        "df2.head(2)"
      ],
      "metadata": {
        "id": "JEwKq-2Qs0g9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "aW93criUAX9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "ngjNrJY9tttE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['lemmatized_Tweet'] = df2['OriginalTweet'].apply(lambda text: [lemmatizer.lemmatize(word) for word in text])\n",
        "df2.head(2)"
      ],
      "metadata": {
        "id": "EQHUiWJ7tWr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used lemmatization as a text normalization technique in the provided code. Lemmatization reduces words to their base or dictionary form (lemma) to standardize the text.\n",
        "\n",
        "Reducing words to their base form: Lemmatization ensures that words are reduced to their base or dictionary form. For example, words like \"running,\" \"ran,\" and \"runner\" are all lemmatized to \"run.\" This can help in recognizing the essential meaning of a word.\n",
        "\n",
        "Better handling of inflected words: Lemmatization handles inflected words better than other techniques like stemming. It provides more accurate and meaningful results by taking into account a word's context and meaning.\n",
        "\n",
        "Improved text analysis: When performing text analysis, lemmatization can lead to more meaningful insights and results. This is especially important for tasks like sentiment analysis, where the correct interpretation of words is crucial."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging\n",
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['POS_Tags'] = df2['lemmatized_Tweet'].apply(lambda text: nltk.pos_tag([nltk.WordNetLemmatizer().lemmatize(word) for word in text]))"
      ],
      "metadata": {
        "id": "QVSGfmndxAMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.head(2)"
      ],
      "metadata": {
        "id": "s6mmyTps0sCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Join the lemmatized words into sentences\n",
        "df2['new_Tweet'].apply(lambda words: ' '.join(words))\n",
        "\n",
        "# Create a CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform the lemmatized column\n",
        "X = vectorizer.fit_transform(df2['new_Tweet'])\n",
        "\n",
        "# Convert it to a DataFrame for visualization\n",
        "X_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "print(X_df)"
      ],
      "metadata": {
        "id": "nzr3olTfYWaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used the Count Vectorization technique. Count Vectorization converts a collection of text documents to a matrix of token counts. Each row in the matrix represents a document from the corpus, and each column represents a unique word (token) in the corpus. The cell values indicate the frequency of each word in the corresponding document.\n",
        "\n",
        "I have used Count Vectorization for the following reasons:\n",
        "\n",
        "- Simplicity: Count Vectorization is straightforward to implement. It represents text data in a format that is easy to understand and work with.\n",
        "\n",
        "- Interpretable Features: Each word becomes a feature in the vectorized data. This allows for easy interpretation and analysis, as you can see the count of each word in each document.\n",
        "\n",
        "- Useful for Machine Learning: Many machine learning algorithms, such as Naive Bayes and logistic regression, work well with count-based features. Count Vectorization can be particularly effective for text classification tasks like sentiment analysis.\n",
        "\n",
        "- Applicability: Count Vectorization is suitable for a wide range of NLP tasks, including document classification, clustering, and information retrieval."
      ],
      "metadata": {
        "id": "YTp5AJDKCmFW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correlation_matrix = df2.corr().abs()\n",
        "correlation_matrix"
      ],
      "metadata": {
        "id": "Egw6Z2EEKfhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(np.bool))\n",
        "upper_triangle"
      ],
      "metadata": {
        "id": "rFORhxKrKnJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.8)]\n",
        "to_drop"
      ],
      "metadata": {
        "id": "5XP0sOJWKuzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Minimize feature correlation by removing highly correlated features\n",
        "to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.8)]\n",
        "df2 = df2.drop(columns=to_drop)"
      ],
      "metadata": {
        "id": "B2Q5eX_pBTwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df2.drop(columns = ['UserName'])"
      ],
      "metadata": {
        "id": "teCNjFZaL8Iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.head(2)"
      ],
      "metadata": {
        "id": "BLvIql-UMZ7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new features based on existing ones\n",
        "df2['Total_Word_Count'] = df2['lemmatized_Tweet'].apply(lambda x: len(x))"
      ],
      "metadata": {
        "id": "TaatTYhsFgIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.head(2)"
      ],
      "metadata": {
        "id": "7ylCJC-MM5ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "# Correlation Heatmap visualization code\n",
        "plt.figure(figsize=(20,8))\n",
        "sns.heatmap(df2.corr(),annot=True,cmap='coolwarm')"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Total_Word_Count , text_length is having positive corelation .so we can drop them.\n",
        "- same as the columns  'Location', 'OriginalTweet', 'text_length', 'month', 'day','Friday', 'Monday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday',\n",
        "'Wednesday', 'lemmatized_Tweet', 'POS_Tags','Total_Word_Count'   these are of no use in future at time of model prediction. so we can drop them.\n"
      ],
      "metadata": {
        "id": "BiMcATH8mlCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2.columns"
      ],
      "metadata": {
        "id": "aQjBLMChERgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.drop(['Location', 'OriginalTweet', 'text_length', 'month', 'day','Sentiment_Num','Friday', 'Monday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday','Wednesday',\n",
        "          'new_Tweet','lemmatized_Tweet', 'POS_Tags','Total_Word_Count'],axis =1,inplace=True)"
      ],
      "metadata": {
        "id": "76FttzM0n07R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.columns"
      ],
      "metadata": {
        "id": "MNUhrxpdmjuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here heatmap is used to visualize the correlation matrix of the features in your dataset. While this doesn't directly perform feature selection, it serves as a valuable step in feature selection and analysis."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "'Sentiment', 'clean_Tweet' these are important features."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No i do not think the data needs to be transformed."
      ],
      "metadata": {
        "id": "ujKKtW4wolaY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import RobustScaler"
      ],
      "metadata": {
        "id": "-5ThbW9dpCTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "# Assigning dependent and independent features\n",
        "X= df2['clean_Tweet']\n",
        "y= df2['Sentiment']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
        "\n",
        "print(\"Shape of X_train : \", X_train.shape)\n",
        "print(\"Shape of y_train : \", y_train.shape)\n",
        "print(\"Shape of X_test : \", X_test.shape)\n",
        "print(\"Shape of y_test : \", y_test.shape)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['clean_Tweet'].isnull().sum()"
      ],
      "metadata": {
        "id": "fZQjISBLpZk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Data Splitting ratio is 80% used for Training Data and 20% used for Test Data.\n",
        "\n",
        "- By providing more data for training, the model can learn more patterns and trends in the data, which can lead to better predictions on new or unseen data."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here no need of any scaling of Dataset becuase here only check sentiment of people,hence we analyze only User sentiment tweet according to the secenario of Covid-19."
      ],
      "metadata": {
        "id": "AyxXK6tbp6jL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reducing dimensionality can lead to improved model performance by reducing the risk of overfitting and making the model more generalizable."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here no need to do for dimensionality reduction."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No dataset is not imbalanced so need of this."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No need for handle imbalance here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Count Vectorization (Bag of words) and TF/IDF Vecorization**"
      ],
      "metadata": {
        "id": "30rw_SMNsDtN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count vectorization is a process of converting a piece of text into a numerical format that can be used by machine learning algorithms. In this process, the text is first split into words or tokens, and then each token is counted to create a vector of numbers representing the frequency of each word in the text."
      ],
      "metadata": {
        "id": "Wo3MdUGLsLE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorization\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "Ymwo0CzWsYlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bag of words\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "X_train = vectorizer.fit_transform(X_train.values)\n",
        "X_test = vectorizer.transform(X_test.values)"
      ],
      "metadata": {
        "id": "eG69nR0msa75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic regression**"
      ],
      "metadata": {
        "id": "0_OUB3iZtmEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error,mean_absolute_error, make_scorer,classification_report,confusion_matrix,accuracy_score,roc_auc_score,roc_curve\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "2LMCWY-e2Fcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "logreg = LogisticRegression()\n",
        "\n",
        "# Fit the Algorithm\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "logreg_prediction = logreg.predict(X_test)\n",
        "\n",
        "logreg_accuracy = accuracy_score(y_test,logreg_prediction)\n",
        "print(\"Training accuracy Score    : \",logreg.score(X_train,y_train))\n",
        "print(\"Validation accuracy Score : \",logreg_accuracy )\n",
        "print(classification_report(logreg_prediction,y_test))"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression is a statistical model that probability of an event taking place by having the the odds against winning range between 0 and 1 or muticlass classfication for the event be a linear combination of one or more independent variables."
      ],
      "metadata": {
        "id": "n5UQRnZY1iZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Plotting Confussion matrix\n",
        "cf1= (confusion_matrix(y_test,logreg_prediction))\n",
        "plt.figure(figsize=(8,5))\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cf1, annot=True, fmt=\".0f\",ax = ax)\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels', fontsize=15)\n",
        "ax.set_ylabel('Actual labels', fontsize=15)\n",
        "ax.set_title('Confusion Matrix (Logistic Regression with CV)', fontsize=20)\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here the algrothim used Logistic Regression with accuracy of 79% and different Evaluation metric Score with labels 'Negative', 'Neutral', Positive like precision ,recall, f1 score is also increased."
      ],
      "metadata": {
        "id": "sDxMymTu4115"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV"
      ],
      "metadata": {
        "id": "ARsoYx5TKK92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Creating a GridSearchCV object with cross-validation of 15\n",
        "# Initalizing the model\n",
        "lr_cv = LogisticRegression()\n",
        "parameters = dict(penalty=['l1', 'l2'],C=[100, 10, 1.0, 0.1, 0.01])\n",
        "\n",
        "#Hyperparameter tuning by GridserchCV\n",
        "logreg_Gcv=GridSearchCV(lr_cv,parameters,cv=15)\n",
        "\n",
        "# Fit the Algorithm\n",
        "logreg_Gcv.fit(X_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "pred_lr_cv = logreg_Gcv.predict(X_test)\n",
        "pred_lr_cv\n",
        "\n",
        "#Accuracy\n",
        "accuracy_lr_cv = accuracy_score(y_test,pred_lr_cv)\n",
        "print(\"Accuracy :\",(accuracy_lr_cv))"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_lr_cv"
      ],
      "metadata": {
        "id": "netyMTSQMSzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(pred_lr_cv,y_test))"
      ],
      "metadata": {
        "id": "17_n9sAFMflJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV is used to tune two hyperparameters for the logistic regression model - the regularization penalty (L1 or L2) and the inverse of regularization strength (C). The performance of the model is evaluated using cross-validation with a cv parameter set to 15."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No, as seen in above Evaluation metric Score Chart as follows\n",
        "\n",
        "Accuracy:-79%\n",
        "\n",
        "precision:-78%\n",
        "\n",
        "recall:- 78%\n",
        "\n",
        "f1-score:- 78%"
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Support vector machines**"
      ],
      "metadata": {
        "id": "70NlrkbrNoIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "# Initializing model\n",
        "svc = SVC()\n",
        "\n",
        "#fitting the data to model\n",
        "svc.fit(X_train, y_train)\n",
        "\n",
        "#prediction\n",
        "svc_prediction = svc.predict(X_test)\n",
        "svc_accuracy = accuracy_score(y_test,svc_prediction)\n",
        "print(\"Training accuracy Score    : \",svc.score(X_train,y_train))\n",
        "print(\"Validation accuracy Score : \",svc_accuracy )\n",
        "print(classification_report(svc_prediction,y_test))"
      ],
      "metadata": {
        "id": "5CVH3r0fN_u1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is a supervised learning algorithm that works by finding the hyperplane that best separates the classes in the input data.\n",
        "\n",
        "In multi-class classification problems, multiple hyperplanes are used to separate the classes. The optimal hyperplane is the one that maximizes the margin, which is the distance between the hyperplane and the closest points from each class, known as support vectors."
      ],
      "metadata": {
        "id": "X6I0UJXuN9XA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "cf6= (confusion_matrix(y_test,svc_prediction))\n",
        "plt.figure(figsize=(8,5))\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cf6, annot=True, fmt=\".0f\",ax = ax)\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels', fontsize=15)\n",
        "ax.set_ylabel('Actual labels', fontsize=15)\n",
        "ax.set_title('Confusion Matrix (SVM with CV)', fontsize=20)\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we have already used in above"
      ],
      "metadata": {
        "id": "6Puc60YMCWw9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "slFWEtnOCWPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "gridsearch"
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No, as seen in above Evaluation metric Score Chart as follows\n",
        "\n",
        "Accuracy:-76%\n",
        "\n",
        "precision:-75%\n",
        "\n",
        "recall:- 74%\n",
        "\n",
        "f1-score:- 74%"
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "according to this model score logistic regression is more good."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Decision Tree Classifier with CV**"
      ],
      "metadata": {
        "id": "n71-SUIN_wLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "metadata": {
        "id": "MCYCjD5xC2zF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "# Initializing model\n",
        "dtc=DecisionTreeClassifier()\n",
        "\n",
        "# Fit the Algorithm\n",
        "dtc.fit(X_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "dtc_prediction = dtc.predict(X_test)\n",
        "dtc_accuracy = accuracy_score(y_test,dtc_prediction)\n",
        "print(\"Training accuracy Score    : \",dtc.score(X_train,y_train))\n",
        "print(\"Validation accuracy Score : \",dtc_accuracy )\n",
        "print(classification_report(dtc_prediction,y_test))"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Plotting Confussion matrix\n",
        "cf2= (confusion_matrix(y_test,dtc_prediction))\n",
        "plt.figure(figsize=(8,5))\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cf2, annot=True, fmt=\".0f\",ax = ax)\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels', fontsize=15)\n",
        "ax.set_ylabel('Actual labels', fontsize=15)\n",
        "ax.set_title('Confusion Matrix (Decision tree with CV)', fontsize=20)\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "i have used CountVectorizer here."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No, as seen in above Evaluation metric Score Chart as follows\n",
        "\n",
        "Accuracy:-71%\n",
        "\n",
        "precision:-71%\n",
        "\n",
        "recall:- 70%\n",
        "\n",
        "f1-score:- 71%"
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RandomForestClassifier with cv**"
      ],
      "metadata": {
        "id": "dN3hfVlemXYR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest works by constructing multiple decision trees on randomly sampled subsets of the training data. Each tree is trained on a different subset of the features and the data, which helps to reduce overfitting and improve the generalization performance.\n",
        "\n",
        "During prediction, the input instance is passed through each decision tree, and the majority vote among the predictions of the individual trees is taken as the final prediction."
      ],
      "metadata": {
        "id": "Z7hHhG7dl_mC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "rf_clf =RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Fit classifier to training data\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "#prediction\n",
        "rf_prediction = rf_clf.predict(X_test)\n",
        "# Calculate accuracy of classifier on test data\n",
        "#accuracy_rf = (y_pred_rf == y_test).mean()\n",
        "#print('Accuracy_rf:', accuracy_rf)\n",
        "rf_accuracy = accuracy_score(y_test,rf_prediction)\n",
        "print(\"Training accuracy Score    : \",rf_clf.score(X_train,y_train))\n",
        "print(\"Validation accuracy Score : \",rf_accuracy )\n",
        "print(classification_report(rf_prediction,y_test))"
      ],
      "metadata": {
        "id": "DzlUn0EAgrHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest algorithm was used to classify the data, resulting in an accuracy of 76%. Additionally, different evaluation metrics, such as precision, recall, and f1 score, were calculated for each label ('Negative', 'Neutral', 'Positive') and not slightly improved compared to the baseline model.\n",
        "\n",
        "Hyperparameter optimization technique has been used for the Random Forest Classifier. The classifier has been instantiated with the hyperparameter values n_estimators=100 and random_state=42, which are default values in the absence of explicit hyperparameter tuning.\n",
        "\n",
        "as seen in above Evaluation metric Score Chart as follows\n",
        "\n",
        "Accuracy:-75%\n",
        "\n",
        "Precision:-75%\n",
        "\n",
        "Recall:- 74%\n",
        "\n",
        "f1-score:-74%"
      ],
      "metadata": {
        "id": "_859-3yzmGAo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Classfication Metrics Report**"
      ],
      "metadata": {
        "id": "ZnQ1sX3ZSU6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification metrics are used to evaluate the performance of a classification model by comparing the predicted labels to the actual labels. Accuracy can be useful in evaluating sentiment analysis models, particularly if the classes are balanced.\n",
        "\n",
        "Accuracy: The proportion of correctly predicted labels out of the total number of samples. It is computed as (TP+TN)/(TP+TN+FP+FN)\n",
        "\n",
        "Where\n",
        "\n",
        "TP:-True Positive\n",
        "\n",
        "TN:-True Negative\n",
        "\n",
        "FP:-False Positive\n",
        "\n",
        "FN:-False Negative"
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acurracy = {'Model':  ['Logistic Regression with GridserachCV','Support-Vector-Machine Classifier', 'Decision Tree Classifier','RandomForest Classifier'],\n",
        "        'Count Vector':  [accuracy_lr_cv,dtc_accuracy,svc_accuracy,rf_accuracy]}\n",
        "\n",
        "cv_score_table= pd.DataFrame (acurracy, columns = ['Model','Count Vector'])\n",
        "cv_score_table"
      ],
      "metadata": {
        "id": "nV3GVjgPSfVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choosing the final prediction model depends on specific problem and the evaluation metric that matters most. It's not solely based on the accuracy but also on other factors like model interpretability, training time, and the specific requirements of your project.\n",
        "\n",
        "Let's briefly discuss each of the models:\n",
        "\n",
        "**Logistic Regression with GridSearchCV (Accuracy: 0.798834):** Logistic Regression is a simple and interpretable model. It performs well when the relationship between features and the target variable is roughly linear. The accuracy is relatively high, but it might not capture complex relationships in the data.\n",
        "\n",
        "**Support-Vector-Machine Classifier (Accuracy: 0.716958):** Support Vector Machines (SVMs) are powerful for handling complex relationships in the data. However, they can be computationally expensive, and the performance depends on choosing the right kernel and hyperparameters. An accuracy of 0.716958 is decent but may not be the highest among the models tested.\n",
        "\n",
        "**Decision Tree Classifier (Accuracy: 0.765185):** Decision Trees are interpretable and can capture non-linear relationships. However, they are prone to overfitting. The accuracy here is moderate.\n",
        "\n",
        "**RandomForest Classifier (Accuracy: 0.755831):** Random Forest is an ensemble of Decision Trees and is known for its robustness and generalization. However, it may not be the most interpretable model."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1-The model that we have used is the Linear Regression with gridsearch cv, which is a linear classifier. The Count Vectorization Technique works by counting the occurrence of each word in the text data and then creating a document-term matrix where each row represents a document and each column represents a word in the vocabulary.\n",
        "\n",
        "2-To understand the feature importance, we can use the Permutation Importance technique provided by the scikit-learn library. The Permutation Importance technique works by randomly permuting the values of a feature and observing the effect on the model's performance."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After evaluating four machine learning models with Count Vectorization for sentiment analysis, we determined the model that best suits our needs:\n",
        "\n",
        "Logistic Regression with Grid Search CV achieved the highest accuracy of 79.88%.\n",
        "The choice of the best model depends on the specific dataset and problem. In this case, Logistic Regression with Grid Search CV demonstrated the highest accuracy for sentiment analysis."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}